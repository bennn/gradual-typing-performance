# Referee: 1

## Abstract: awkward phrase

> "currently, this assurance requires run-time checks"... will it ever not?
> There is a name for when dynamic checks aren't required: static typing.

Removed the word "currently".


## Section 1: explain "the problem"

> For Twitter, the problems with Ruby were (a) performance, and
> (b) code anti-patterns

> It's misleading to say on p1 that the "server-side application" of Twitter
> is written in Ruby... some of the front-end _may_ still be written in
> Ruby, but my impression is that very little remains.

???


## Section 1: address Node.js

> worth adding that for many developers, the default for new services is to
> use JavaScript on Node.

???


## Section 1: defend 'migratory typing'

> if the authors feel strongly that migratory typing is a better name,
> then please actually explain the reasoning and argue for it

Replaced the footnote with one that reads:

  "Others [SNAPL'17] refer to this use of gradual typing as _migratory typing_"

Where SNAPL'17 is a citation to "Migratory Typing: Ten years later" by
 Tobin-Hochstadt et al.


## Section 2.1: clarify why this performance overhead is a problem

> explain why this unpredictability is unsatisfactory in the gradual typing
> context

???


## Section 2.2: strengthen argument (or remove)

> the short treatment given here isn't so convincing... which weakens the
> case.

> showing examples from other languages that purport to have gradual
> typing but are unsound will strengthen the case significantly.

> [the case should address] those the developers who ask for flags to
> disable contract checking

> give more, stronger examples

> what kind of unsoundnesses can happen?

???


## Section 2.2: fix awkward 'recall' about types

> From where should we "recall that types are checkable statements about
> program expressions"? Checkable seems to imply "dynamically checkable",
> but not all types can be checked dynamically (e.g., function types can be
> checked on a finite portion of their domain, but not always exhaustively).
> There are systems where type checking is in general undecidable... do
> these systems have types, or something else?

Removed 'checkable', clarified the sentence that follows.
Thank you for catching our mistake!


## Section 3.1: explain where lattice lines _should_ go

> It's slightly jarring to see something without lines between nodes called
> a "lattice"

Added lines between lattice nodes.


## Section 3.3: explain `k`, the number of steps

> It took several read-throughs to realize that both graphs depict _all_
> configurations, when k=0 and when k=1

Reorganized section 3.3 to clarify the contents of the graphs.
The revised section says:

1. the goal is to plot k-step D-deliverable configurations
2. if k is fixed, choosing different D gives a histogram (for the whole lattice)
3. the graphs fix k=0 and k=1, respectively


## Section 5.1: why random permutation

> why random permutations of configurations?

No change to the paper.

We measure the configurations in a random order to control for confounding
 variables, such as OS-level caching.
We do not have evidence that measuring the configurations in a fixed order
 will affect the measurements; randomizing the order is just a precaution.


## Section 5.1: why drop the first run?

> Is there some bytecode/compilation/cache effect that's relevant?

Added note about the JIT compiler to Section 5.1.

Yes, Racket has a bytecode compiler and a JIT compiler. See:
 `http://docs.racket-lang.org/guide/performance.html#%28part._.J.I.T%29`


##  Section 5.1: define number of iterations

> what is N, i.e., how many times does each benchmark run?

Changed `N` to `N >= 10`
 and clarified the last paragraph of Section 5.1 to say that the appendix also
 lists running times.


## Section 5.1: compute-bound? I/O-bound?

> Are these benchmarks compute or I/O bound?

Added "reading from a file" as a threat to validity in Section 7.

We have found no evidence that the benchmarks are compute, I/O, or memory-bound.


## Section 5.1: define green threads; do all threads block on I/O?

> Footnote 12 mentions green threads without defining them or clarifying
> whether all threads block on I/O.

Removed the phrase "green thread", simplified the description in the footnote.


## Section 7: change prose before Dimoulas etal 2012 citation

> The citation to Dimoulas et al. 2012 on p24 as saying blame offers
> "practical value for developers" is misleading---that paper is a
> theoretical reasoning framework, with no evaluation on whether developers
> find blame practical

Moved the citation to the phrase 'complete monitoring', slightly revised the
 paragraph to fit.


## Section 7: clarify final line

> ...  is really a question about reconciling a desire for
> (a) blame,
> (b) soundness,
> (c) new typed libraries efficiently using legacy untyped Racket code, and
> (d) efficient new untyped Racket code using new typed libraries.

Changed the final line to say that the alternative soundness appears to be
 more performant, but offers less help for debugging type boundary errors.


## Ballantyne's example

> Ballantyne also offers _much_ worse slowdown: 1275x!
> Why isn't this example included in the paper?

Added a citation to Section 8.3.

This example definitely belongs in the paper.

It is not one of the benchmarks because (1) the source of performance
 overhead is clear and (2) we only have one partially-typed configuration for it.


## compare to TypeScript

> Rastogi et al. POPL 2015 report a 1.15x slowdown... what's different for
> racket, or is it just because they're only reporting the topmost node in
> the performance lattice?

Added a citation to Rastogi et al. POPL 2015 to the conclusion,
 as one example for how keeping run-time type information might improve performance.

It is difficult to compare to Safe TypeScript based on the ad-hoc evaluation in
 Rastogi et al. POPL 2015.
Besides the 1.15x number, they report an average slowdown of 22x for the
 fully-untyped configuration of six Octane benchmarks programs.
We hope that our paper inspires a in-depth comparison.


## compare to other space-efficiency papers

> Herman 2007 TFP
> Greenberg's TFP 2016
> Garcia 2013 ICFP (less close)
> Siek et al. ESOP 2015 (less close)

Added a citation to Siek et al. ESOP 2015 to the conclusion,
 as a second example of using run-time type information to improve performance.

We cite the journal version of Herman et al.'s 2007 paper and Greenberg's POPL
 2015 paper (Space-Efficient Manifest Contracts) in Section 9.


## compare to Reticulated

>  mention Vitousek etal, POPL 2017

Added a brief comparison to Section 7 and Section 9.


## References to particular sections/figures/etc. should be capitalized

Not according to the Chicago Manual of Style Online (3.9).


## use a single format for speedups

> 'i.e., .7x vs. 30% performance improvement'

Replaced all percentages with overhead factors.


# Referee: 2

## Section 1: cite the papers we accuse of poor evaluations

> It is inappropriate in scientific literature to make accusations without
> citing the accused papers, otherwise accusations cannot be checked and
> debated.

Added citations.


## Section 2: explain natural embedding

> The performance evaluation of Typed Racket cannot be understood from a
> scientific perspective without understanding those aspects of the
> implementation of Typed Racket that influence performance.

(Referee 3 also wants explanation)

???


## Section 3: clarify 'experimental vs. fixed' modules

> some programs, even in their 100% statically typed configuration, rely on
> libraries that are untyped. This is an inaccurate presentation of the
> data.

Added two definitions to clarify this part of the method.
Full changes:

- Section 3.1: define _experimental modules_ and _fixed modules_
  (experimental modules define the lattice, fixed modules are part of the
   program but not part of the experiment e.g. because they are part of the
   Racket runtime library)
- Section 4.0: explain that the 'Depends' field in the benchmark descriptions
  lists libraries of _fixed modules_
- Section 4.0: clarify that `# MOD` in Figure 6 is the number of _experimental
  modules_
- Section 4.1: state that adaptor modules are fixed modules
- Section 6.0: clarify that the method is limited by the number of _experimental
  modules_


## Section 4: define GTP

> The acronym GTP needs to be defined.

Added a definition.


## Section 4.1: typo, missing period

> Seventeen of the benchmark programs are adaptations of untyped
> programs The other

Fixed.


# Referee: 3

## Section 2: explain natural embedding

> boxing/unboxing overheads are not discussed

> I have always assumed that the majority of the cost of crossing
> boundaries in gradual typing (in a compiled implementation) would come
> from boxing (mallocs) and not tag checking (conditionals in generally
> should be very cheap at machine code level).

> state clearly:
> 1) if the implementation boxes/unboxes values based on
>     whether they are untyped/typed
> 2) if the implementation is interpreted or compiled (to machine doe)
> 3) if that is done, whether there has been any effort to evaluate the
>    cost of the dynamic allocation in this work

???


## Section 8: add experimental support

> It is unfortunate that the paper does not attempt to experimentally
> establish support for these hypothesis.

???

