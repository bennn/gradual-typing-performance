General Response
================================================================================

In response to the referees' comments about section 2 (Gradual Typing in Typed
 Racket), we have significantly re-organized the section.
Section 2 in the original submission followed this outline:

- 2   : brief introduction to Typed Racket
        + reasons for migrating from Racket to Typed Racket
- 2.1 : reasons for using both Racket and Typed Racket
        (rather than migrating the entire application to Racket)
- 2.2 : case for type soundness

Section 2 in the revised submission follows a modified outline:

- 2   : brief introduction to Typed Racket
- 2.1 : reasons for migrating from Racket to Typed Racket
        + reasons to continue using Racket
- 2.2 : anecdotes suggesting a need for performance evaluation

Summary: we removed the argument for type soundness and instead describe
 the history that led to this work on performance evaluation.

- - -

The rest of this document addresses specific comments from each referee;
 sections are labeled "Referee: 1", "Referee: 2", and "Referee: 3".

Each response has the format:

```
  ## SECTION: TITLE

  > CLARIFICATION

  DELTA

  COMMENT
```

where:
- `SECTION` is the relevant section of the paper
- `TITLE` describes the issue
- `CLARIFICATION` quotes the referee
- `DELTA` describes any edits to the paper
- `COMMENT` is any extra reply to the referee


Referee: 1
================================================================================

## Abstract: awkward phrase

> "currently, this assurance requires run-time checks"... will it ever not?
> There is a name for when dynamic checks aren't required: static typing.

Removed the word "currently".


## Section 1: Twitter does not currently use Ruby

> It's misleading to say on p1 that the "server-side application" of Twitter
> is written in Ruby... some of the front-end _may_ still be written in
> Ruby, but my impression is that very little remains.

Changed to use past-tense ("were originally written").


## Section 1: address Node.js

> worth adding that for many developers, the default for new services is to
> use JavaScript on Node.

The opening paragraph now cites Node.js


## Section 1: explain "the problem"

> The third paragraph of the intro refers to "the problem" without saying what
> the problem is. For Twitter, the problems with Ruby were (a) performance, and
> (b) code anti-patterns

Revised this paragraph to clarify Twitter's issues with Ruby.


## Section 1: defend 'migratory typing'

> if the authors feel strongly that migratory typing is a better name,
> then please actually explain the reasoning and argue for it

Replaced the footnote with one that reads:

  "[SNAPL'17] refer to this use of gradual typing as _migratory typing_"

Where SNAPL'17 is a citation to "Migratory Typing: Ten years later" by
 Tobin-Hochstadt et al.


## Section 2.1: clarify why this performance overhead is a problem

> On some level, the results shown here---that applying sound gradual types to
> only part of a program can _severely_ degrade performance---are unsurprising:
> plenty of things that are supposed to make programs run faster actually don't!

> ... explain why this unpredictability is unsatisfactory in the gradual typing
> context

Clarified (in the new sec 2.2) that the unpredictability goes against the
 intuition that adding types should improve performance.


## Section 2.2: strengthen argument (or remove)

> the short treatment given here isn't so convincing... which weakens the
> case.

> showing examples from other languages that purport to have gradual
> typing but are unsound will strengthen the case significantly.

> [the case should address] those the developers who ask for flags to
> disable contract checking

Removed the argument.


## Section 2.2: fix awkward 'recall' about types

> From where should we "recall that types are checkable statements about
> program expressions"? Checkable seems to imply "dynamically checkable",
> but not all types can be checked dynamically (e.g., function types can be
> checked on a finite portion of their domain, but not always exhaustively).
> There are systems where type checking is in general undecidable... do
> these systems have types, or something else?

Removed the phrase.

[[ Saying 'checkable' was an error in our submission.
   Thank you for catching it. ]]


## Section 3.1: add lattice lines

> It's slightly jarring to see something without lines between nodes called
> a "lattice"

Added lines between lattice nodes.


## Section 3.3: explain `k`, the number of steps

> It took several read-throughs to realize that both graphs depict _all_
> configurations, when k=0 and when k=1

Reorganized section 3.3 to clarify the contents of the graphs.
The revised section says:

1. the goal is to plot k-step D-deliverable configurations
2. if k is fixed, choosing different D gives a histogram for the whole lattice
3. the graphs fix k=0 and k=1, respectively


## Section 5.1: why random permutation

> why random permutations of configurations?

Added a footnote to clarify.

We randomize as a precaution against possible confounding effects
 (for example, OS-level caching).


## Section 5.1: why drop the first run?

> Is there some bytecode/compilation/cache effect that's relevant?

Clarified that we drop the first run to control for JIT compiler warmup.


##  Section 5.1: define number of iterations

> what is N, i.e., how many times does each benchmark run?

Changed `N` to `N >= 10` and clarified the last paragraph of Section 5.1 to
 say that the appendix also lists the number of iterations.


## Section 5.1: compute-bound? I/O-bound?

> Are these benchmarks compute or I/O bound?

Added "reading from a file" as a threat to validity in Section 7.

In the benchmarks that do perform I/O actions, those actions take only a small
 fraction of the total running time.


## Section 5.1: define green threads; do all threads block on I/O?

> Footnote 12 mentions green threads without defining them or clarifying
> whether all threads block on I/O.

Removed the phrase "green thread", simplified the description in the footnote.


## Section 7: change prose before Dimoulas etal 2012 citation

> The citation to Dimoulas et al. 2012 on p24 as saying blame offers
> "practical value for developers" is misleading---that paper is a
> theoretical reasoning framework, with no evaluation on whether developers
> find blame practical

Moved the citation to the phrase 'complete monitoring', revised the paragraph.


## Section 7: clarify final line

> ...  is really a question about reconciling a desire for
> (a) blame,
> (b) soundness,
> (c) new typed libraries efficiently using legacy untyped Racket code, and
> (d) efficient new untyped Racket code using new typed libraries.

Changed the final line to say that the alternative soundness imposes less
 overhead, but offers less help for debugging type boundary errors.


## Ballantyne's example

> Ballantyne also offers _much_ worse slowdown: 1275x!
> Why isn't this example included in the paper?

Added a citation to Section 8.3.

This example is not one of the benchmarks because (1) the source of performance
 overhead is similar to other benchmarks, like `fsm`, and (2) we only have one
 partially-typed configuration for it.


## compare to TypeScript

> Rastogi et al. POPL 2015 report a 1.15x slowdown... what's different for
> racket, or is it just because they're only reporting the topmost node in
> the performance lattice?

Added a citation to Rastogi et al. POPL 2015 to the conclusion as one example
 for how keeping run-time type information might improve performance.

It is difficult to compare to SafeTypeScript based on the POPL 2015 paper.
The 1.15x number is quite low, but they also report an average slowdown of
 22x on six untyped programs from the Octane benchmarks.
We hope that this journal paper inspires a detailed comparison.


## compare to other space-efficiency papers

> Herman 2007 TFP
> Greenberg's TFP 2016
> Garcia 2013 ICFP (less close)
> Siek et al. ESOP 2015 (less close)

Added a citation to Siek et al. ESOP 2015 to the conclusion, as a second
example of using run-time type information to improve performance.

We cite the journal version of Herman et al.'s 2007 paper and Greenberg's POPL
 2015 paper (Space-Efficient Manifest Contracts) in Section 9.


## compare to Reticulated

>  mention Vitousek etal, POPL 2017

Added a brief comparison to Section 7 and Section 9.


## References to particular sections/figures/etc. should be capitalized

Not according to the Chicago Manual of Style Online (3.9).


## use a single format for speedups

> 'i.e., .7x vs. 30% performance improvement'

Replaced all percentages with overhead factors.


# Referee: 2
================================================================================

## Section 1: cite the papers we accuse of poor evaluations

> It is inappropriate in scientific literature to make accusations without
> citing the accused papers, otherwise accusations cannot be checked and
> debated.

Added citations.


## Section 3: clarify the role of library modules

> some programs, even in their 100% statically typed configuration, rely on
> libraries that are untyped. This is an inaccurate presentation of the
> data.

Added two definitions to clarify this part of the method.
Full changes:

- Section 3.1: define _migrated modules_ and _unchanged modules_
  (migrated modules define the lattice, unchanged modules are part of the
   program but not part of the experiment e.g. because they are part of the
   Racket runtime library)
- Section 4: explain that the 'Depends' field in the benchmark descriptions
  lists libraries of _unchanged modules_
- Section 4: clarify that `# MOD` in Figure 6 is the number of _migrated
  modules_
- Section 4.1: state that adaptor modules are fixed modules
- Section 6: clarify that the method of section 3 is limited to programs
  with roughly 20 _migrated modules_

Note that some programs rely on typed libraries, i.e., the `lnm` benchmark.


## Section 4: define GTP

> The acronym GTP needs to be defined.

Added a definition.


## Section 4.1: typo, missing period

> Seventeen of the benchmark programs are adaptations of untyped
> programs The other

Fixed.


## Section 8: explain how Typed Racket enforces soundness

> The performance evaluation of Typed Racket cannot be understood from a
> scientific perspective without understanding those aspects of the
> implementation of Typed Racket that influence performance.

Revised sec. 8 to explain how Typed Racket enforces soundness, and how this
 enforcement leads to performance overhead.

[[ cc Referee 3 ]]


# Referee: 3
================================================================================

## Section 2: explain how Typed Racket enforces soundness

> boxing/unboxing overheads are not discussed

> I have always assumed that the majority of the cost of crossing
> boundaries in gradual typing (in a compiled implementation) would come
> from boxing (mallocs) and not tag checking (conditionals in generally
> should be very cheap at machine code level).

> state clearly:
> 1) if the implementation boxes/unboxes values based on
>     whether they are untyped/typed
> 2) if the implementation is interpreted or compiled (to machine doe)
> 3) if that is done, whether there has been any effort to evaluate the
>    cost of the dynamic allocation in this work

Revised sec. 8 to explain how Typed Racket enforces soundness.

We believe the three points above are addressed in the revised version of the
 paper, but just to be clear:

1. The implementation does not box/unbox values based on whether they are
   typed/untyped, but it does box (in the sense of wrapping with a proxy)
   some untyped values sent to typed code and some typed values sent to untyped
   code. These are values that cannot be fully-checked at the boundary; for
   example, mutable data, functions, and objects get proxied.
2. The implementation is compiled to bytecode and JIT-compiled to machine code.
3. In this work, we do not measure the cost of runtime mallocs.


## Section 8: add experimental support

> It is unfortunate that the paper does not attempt to experimentally
> establish support for these hypothesis.

No change to the paper.

Section 8 does have ad-hoc evidence for how the patterns affect the benchmarks.
Figure 24 in the appendix has some additional evidence.

Here is a listing of all the evidence we provide, ignoring the code samples:

- Section 8.1: High-Frequency Typechecking
  * 6 million values flow across 4 boundaries in snake
  * 100 million values flow across 2 boundaries in suffixtree
  * the "Max Boundary" column in fig. 24
- Section 8.2: High-Cost Types
  * 19.4x slowdown in fully-typed quadMB from a type-generated predicate
  * 3x slowdown due to hashtable types
- Section 8.3: Complex Type Boundaries
  * no evidence
- Section 8.4: Layered Proxies
  * 235x slowdown in fsm due to layered proxies
  * 27x slowdown in forth due to layered proxies
  * 300x slowdown in zombie due to layered proxies
  * the "Max Wraps" column in fig. 24
- Section 8.5: Library Boundaries
  * 2x slowdown in mbta due to an untyped library
  * 4x slowdown in zordoz due to an untyped library
  * the "Library %" column in fig. 24
